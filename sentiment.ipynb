{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "df = pd.read_csv('sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                       0\n",
      "title                    0\n",
      "body                     0\n",
      "notification_priority    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize text data by converting to lowercase\n",
    "df['title'] = df['title'].str.lower()\n",
    "df['body'] = df['body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted characters and punctuation\n",
    "df['title'] = df['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['body'] = df['body'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "यो एउटा सानो परीक्षण हो। यसले शब्द विभाजन गर्न मद्दत पुर्‍याउँछ।\n",
      "\n",
      "Tokens:\n",
      "['यो', 'एउटा', 'सानो', 'परीक्षण', 'हो', '।', 'यसले', 'शब्द', 'विभाजन', 'गर्न', 'मद्दत', 'पुर्\\u200dयाउँछ', '।']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "# Nepali text for tokenization\n",
    "sample_text = \"यो एउटा सानो परीक्षण हो। यसले शब्द विभाजन गर्न मद्दत पुर्‍याउँछ।\"\n",
    "\n",
    "# Tokenizing the text into words\n",
    "tokens = list(indic_tokenize.trivial_tokenize(sample_text))\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for Line 1: ['यो', 'एउटा', 'सानो', 'परीक्षण', 'हो', '।']\n",
      "Tokens for Line 2: ['म', 'मौसम', 'सुन्दर', 'भएकोले', 'खुशी', 'छु', '।']\n",
      "Tokens for Line 3: ['शिक्षा', 'सबैका', 'लागि', 'अनिवार्य', 'हुनुपर्छ', '।']\n"
     ]
    }
   ],
   "source": [
    "# Multiple lines of Nepali text\n",
    "texts = [\n",
    "    \"यो एउटा सानो परीक्षण हो।\",\n",
    "    \"म मौसम सुन्दर भएकोले खुशी छु।\",\n",
    "    \"शिक्षा सबैका लागि अनिवार्य हुनुपर्छ।\"\n",
    "]\n",
    "\n",
    "# Tokenize each line\n",
    "tokenized_texts = [list(indic_tokenize.trivial_tokenize(text)) for text in texts]\n",
    "\n",
    "# Print tokenized lines\n",
    "for i, tokens in enumerate(tokenized_texts, 1):\n",
    "    print(f\"Tokens for Line {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   id                               title  \\\n",
      "0   1  फाइभजी मा प्रधानमन्त्रीको नयाँ कदम   \n",
      "1   2   नेपाल मा प्रधानमन्त्रीको नयाँ कदम   \n",
      "2   3               चीन फाइनल खेलको तयारी   \n",
      "3   4          चीन सम्बन्धमा नयाँ मस्यौदा   \n",
      "4   5  एआई ले नयाँ उपकरण सार्वजनिक गर्‍यो   \n",
      "\n",
      "                                                body  notification_priority  \n",
      "0  नेपाल को भ्रमणले आर्थिक विकासको नयाँ ढोका खोल्...                      1  \n",
      "1  क्रिकेट को निर्णयले व्यापक प्रतिक्रिया ल्याएको छ।                      1  \n",
      "2   ओलम्पिक प्रतियोगितामा नेपालको ऐतिहासिक प्रदर्शन।                      0  \n",
      "3    नेपाल को निर्णयले व्यापक प्रतिक्रिया ल्याएको छ।                      1  \n",
      "4  ओलम्पिक मा एआई प्रविधिको प्रयोगले सुधार ल्याउन...                      0  \n",
      "\n",
      "Dataset with Tokenized Columns:\n",
      "   id                               title_tokens  \\\n",
      "0   1   [फाइभजी, मा, प्रधानमन्त्रीको, नयाँ, कदम]   \n",
      "1   2    [नेपाल, मा, प्रधानमन्त्रीको, नयाँ, कदम]   \n",
      "2   3                 [चीन, फाइनल, खेलको, तयारी]   \n",
      "3   4            [चीन, सम्बन्धमा, नयाँ, मस्यौदा]   \n",
      "4   5  [एआई, ले, नयाँ, उपकरण, सार्वजनिक, गर्‍यो]   \n",
      "\n",
      "                                         body_tokens  notification_priority  \n",
      "0  [नेपाल, को, भ्रमणले, आर्थिक, विकासको, नयाँ, ढो...                      1  \n",
      "1  [क्रिकेट, को, निर्णयले, व्यापक, प्रतिक्रिया, ल...                      1  \n",
      "2  [ओलम्पिक, प्रतियोगितामा, नेपालको, ऐतिहासिक, प्...                      0  \n",
      "3  [नेपाल, को, निर्णयले, व्यापक, प्रतिक्रिया, ल्य...                      1  \n",
      "4  [ओलम्पिक, मा, एआई, प्रविधिको, प्रयोगले, सुधार,...                      0  \n",
      "\n",
      "Tokenized dataset saved as 'tokenized_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "# Replace 'your_dataset.csv' with your actual file name\n",
    "df = pd.read_csv('sentiment.csv')\n",
    "\n",
    "# Preview the dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Tokenization for the 'title' and 'body' columns\n",
    "df['title_tokens'] = df['title'].apply(lambda x: list(indic_tokenize.trivial_tokenize(x)))\n",
    "df['body_tokens'] = df['body'].apply(lambda x: list(indic_tokenize.trivial_tokenize(x)))\n",
    "\n",
    "# Display the tokenized columns\n",
    "print(\"\\nDataset with Tokenized Columns:\")\n",
    "print(df[['id', 'title_tokens', 'body_tokens', 'notification_priority']].head())\n",
    "\n",
    "# Save the updated dataset to a new CSV file\n",
    "df.to_csv('tokenized_dataset.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nTokenized dataset saved as 'tokenized_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
